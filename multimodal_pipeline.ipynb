{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ab4c5e4",
   "metadata": {},
   "source": [
    "# Multimodal Image-to-Speech Pipeline\n",
    "\n",
    "**Workflow:** Input Image → Object Detection → Label Extraction & Counting → Text Generation → Text-to-Speech → Audio Output\n",
    "\n",
    "**Models Used:**\n",
    "- **Object Detection:** `facebook/detr-resnet-50` (DETR - Transformer-based detector)\n",
    "- **Text-to-Speech:** `suno/bark-small` (Neural TTS)\n",
    "\n",
    "**Text Generation:** Pure Python logic (no LLM needed — simple label counting and sentence construction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887a8b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install transformers torch Pillow scipy timm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83e2158",
   "metadata": {},
   "source": [
    "## Step 1: Object Detection\n",
    "\n",
    "Detect objects in the image using `facebook/detr-resnet-50` (DETR - DEtection TRansformer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526c36c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "from scipy.io.wavfile import write as write_wav\n",
    "from IPython.display import Audio as IPythonAudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2ff273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "image_path = \"images/image_1.jpg\"\n",
    "image = Image.open(image_path)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea43163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run object detection\n",
    "detector = pipeline(\n",
    "    task=\"object-detection\",\n",
    "    model=\"facebook/detr-resnet-50\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "detections = detector(image)\n",
    "\n",
    "# Display results\n",
    "for det in detections:\n",
    "    print(f\"  {det['label']}: {det['score']:.2f}\")\n",
    "\n",
    "print(f\"\\nTotal: {len(detections)} object(s) detected\")\n",
    "\n",
    "# Free memory\n",
    "del detector\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833ced02",
   "metadata": {},
   "source": [
    "## Step 2: Label Extraction & Text Generation\n",
    "\n",
    "Count occurrences of detected objects and build a natural language sentence. Pure Python logic — no LLM needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7af89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each label\n",
    "label_counts = Counter(det[\"label\"] for det in detections)\n",
    "print(\"Label counts:\", dict(label_counts))\n",
    "\n",
    "# Build a descriptive sentence\n",
    "parts = []\n",
    "for label, count in label_counts.items():\n",
    "    if count == 1:\n",
    "        parts.append(f\"1 {label}\")\n",
    "    else:\n",
    "        parts.append(f\"{count} {label}s\")\n",
    "\n",
    "if len(parts) == 0:\n",
    "    text = \"No objects were detected in the image.\"\n",
    "elif len(parts) == 1:\n",
    "    text = f\"The image contains {parts[0]}.\"\n",
    "elif len(parts) == 2:\n",
    "    text = f\"The image contains {parts[0]} and {parts[1]}.\"\n",
    "else:\n",
    "    text = f\"The image contains {', '.join(parts[:-1])}, and {parts[-1]}.\"\n",
    "\n",
    "print(f\"\\nGenerated text: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07357387",
   "metadata": {},
   "source": [
    "## Step 3: Text-to-Speech (TTS)\n",
    "\n",
    "Convert the generated text into speech audio using `suno/bark-small` and save as a WAV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97669b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run text-to-speech\n",
    "synthesizer = pipeline(\n",
    "    task=\"text-to-speech\",\n",
    "    model=\"suno/bark-small\",\n",
    ")\n",
    "\n",
    "result = synthesizer(text)\n",
    "\n",
    "# Extract audio data\n",
    "audio = np.array(result[\"audio\"][0])\n",
    "sampling_rate = result[\"sampling_rate\"]\n",
    "\n",
    "# Normalize and save as WAV\n",
    "audio = audio / np.max(np.abs(audio))\n",
    "audio_16bit = (audio * 32767).astype(np.int16)\n",
    "\n",
    "output_path = \"output/output.wav\"\n",
    "write_wav(output_path, sampling_rate, audio_16bit)\n",
    "print(f\"Audio saved to: {output_path}\")\n",
    "print(f\"Sampling rate: {sampling_rate} Hz\")\n",
    "\n",
    "# Free memory\n",
    "del synthesizer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eae76f1",
   "metadata": {},
   "source": [
    "## Play Audio Output\n",
    "\n",
    "Listen to the generated speech directly in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141fdfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play the audio inline\n",
    "IPythonAudio(audio_16bit, rate=sampling_rate)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
