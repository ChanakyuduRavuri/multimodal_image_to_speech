{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2797cbff",
   "metadata": {},
   "source": [
    "# Multimodal Image-to-Speech Pipeline — Approach 2\n",
    "\n",
    "**Using Google GenAI (Gemini) for Image Captioning + HuggingFace Transformers Pipeline for TTS**\n",
    "\n",
    "## System Architecture\n",
    "\n",
    "```\n",
    "Input Image\n",
    "    ↓\n",
    "Image Captioning / Object Detection (Google GenAI — gemini-3-flash-preview)\n",
    "    ↓\n",
    "Generated Text\n",
    "    ↓\n",
    "Text Processing (Optional Enhancement)\n",
    "    ↓\n",
    "Text-to-Speech Model (suno/bark-small)\n",
    "    ↓\n",
    "Audio Output\n",
    "```\n",
    "\n",
    "## Models Used\n",
    "\n",
    "| Task | Model | Provider |\n",
    "|------|-------|----------|\n",
    "| Image Captioning | `gemini-3-flash-preview` | Google GenAI |\n",
    "| Text-to-Speech | `suno/bark-small` | HuggingFace Transformers |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e8fead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-genai transformers torch Pillow scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc59cd44",
   "metadata": {},
   "source": [
    "## Step 1: Image Captioning using Google GenAI SDK\n",
    "\n",
    "**Objective:** Generate a detailed, accessibility-focused caption describing the image.\n",
    "\n",
    "Instead of detecting isolated objects (like DETR in Approach 1), the Gemini vision model:\n",
    "- Understands the entire scene\n",
    "- Identifies relationships between objects\n",
    "- Describes actions and context\n",
    "- Produces natural, human-like language\n",
    "\n",
    "**Model:** `gemini-3-flash-preview`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aef3215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "from scipy.io.wavfile import write as write_wav\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324b0df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Gemini API key from local file\n",
    "f = open(\"keys/.gemini.txt\")\n",
    "key = f.read().strip()\n",
    "f.close()\n",
    "\n",
    "client = genai.Client(api_key=key)\n",
    "print(\"Gemini client initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13cfee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display the input image\n",
    "image_path = \"images/image_1.jpg\"\n",
    "image = Image.open(image_path)\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e911ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image bytes and send to Gemini for captioning\n",
    "with open(image_path, \"rb\") as f:\n",
    "    image_bytes = f.read()\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a helpful AI Assistant. Given an image perform object detection \"\n",
    "    \"and provide a text output which contains the information about the labels \"\n",
    "    \"detected and their counts.\"\n",
    ")\n",
    "\n",
    "contents = [\n",
    "    types.Part.from_bytes(data=image_bytes, mime_type=\"image/jpeg\"),\n",
    "]\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-3-flash-preview\",\n",
    "    contents=contents,\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=SYSTEM_PROMPT,\n",
    "    ),\n",
    ")\n",
    "\n",
    "raw_caption = response.text\n",
    "print(\"Gemini Response:\")\n",
    "print(raw_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09093e25",
   "metadata": {},
   "source": [
    "## Step 2: Text Processing (Optional Enhancement)\n",
    "\n",
    "**Objective:** Prepare the generated caption for speech synthesis.\n",
    "\n",
    "Possible enhancements:\n",
    "- Remove unnecessary symbols (markdown formatting, etc.)\n",
    "- Control length (brief/detailed mode)\n",
    "- Adjust tone (formal/informal)\n",
    "- Add introductory phrase (e.g., *\"Here is what I see in the image...\"*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28fb37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the raw caption for TTS\n",
    "\n",
    "# Remove markdown-style formatting symbols\n",
    "text = re.sub(r\"[*_#`>]\", \"\", raw_caption)\n",
    "\n",
    "# Collapse multiple whitespace / newlines into a single space\n",
    "text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "# Add an introductory phrase\n",
    "if not text.lower().startswith(\"here is\"):\n",
    "    text = \"Here is what I see in the image: \" + text\n",
    "\n",
    "# Truncate to ~500 chars to keep TTS output manageable\n",
    "if len(text) > 500:\n",
    "    text = text[:497] + \"...\"\n",
    "\n",
    "print(\"Processed text for TTS:\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcf5704",
   "metadata": {},
   "source": [
    "## Step 3: Text-to-Speech (TTS)\n",
    "\n",
    "**Objective:** Convert the generated descriptive text into natural speech audio.\n",
    "\n",
    "**Model:** `suno/bark-small`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bb2d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run TTS pipeline\n",
    "synthesizer = pipeline(\n",
    "    task=\"text-to-speech\",\n",
    "    model=\"suno/bark-small\",\n",
    ")\n",
    "\n",
    "result = synthesizer(text)\n",
    "\n",
    "# Extract audio data and sampling rate\n",
    "audio = np.array(result[\"audio\"][0])\n",
    "sampling_rate = result[\"sampling_rate\"]\n",
    "\n",
    "# Normalize audio to 16-bit PCM range\n",
    "audio = audio / np.max(np.abs(audio))\n",
    "audio_16bit = (audio * 32767).astype(np.int16)\n",
    "\n",
    "# Save as WAV\n",
    "import os\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "write_wav(\"output/output_v2.wav\", sampling_rate, audio_16bit)\n",
    "print(f\"Audio saved to: output/output_v2.wav\")\n",
    "print(f\"Sampling rate: {sampling_rate} Hz\")\n",
    "\n",
    "# Clean up\n",
    "del synthesizer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321a04e4",
   "metadata": {},
   "source": [
    "## Play Audio Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7523a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_16bit, rate=sampling_rate)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
